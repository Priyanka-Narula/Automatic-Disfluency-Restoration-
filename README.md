# Automatic-Disfluency-Restoration-
Challenge is to build a system that can predict the original, disfluent transcript from a clean text and its corresponding audio
##üéß Automatic Disfluency Restoration:
Recreating the Nuance of Human SpeechThis project addresses the challenging task of Automatic Disfluency Restoration, which sits at the intersection of Automatic Speech Recognition (ASR) and Natural Language Generation (NLG). The goal is to build a robust system that can predict the original, verbatim (disfluent) transcript from a given clean text and its corresponding audio recording.This work requires models to understand not just what was said, but also how it was said, including the natural pauses, hesitations, and filler words that are an integral part of human communication.The Core Technical ChallengeThe primary challenge is a multi-modal sequence-to-sequence prediction problem:$$\text{Model}(\text{Clean Transcript}, \text{Audio}) \rightarrow \text{Verbatim Transcript (with Disfluencies)}$$This emphasizes the following key areas:Multi-modal Fusion: Developing an effective strategy to combine information from both the text (clean transcript) and the audio signal to make nuanced predictions.Data Preprocessing as a Skill: The ability to programmatically create target training labels ("clean" transcripts) from raw, disfluent data using a provided list of common disfluencies. This is a critical real-world skill in deep learning projects.Model Selection: Choosing and fine-tuning appropriate models (e.g., ASR or sophisticated sequence-to-sequence architectures) capable of handling the complexities of spoken language, especially within limited computational resources.Project Focus & DeliverablesThis project is an exercise in practical deep learning skills, requiring participants to:Design a robust data cleaning pipeline to map raw, disfluent transcripts to clean inputs.Select and fine-tune appropriate models for a speech task.Develop an end-to-end system that processes both audio and text to generate the final verbatim transcript.üóÉÔ∏è Dataset Description (Hindi Audio)The dataset consists of audio clips in Hindi, their corresponding disfluent transcripts, and a list of common disfluencies. The system must use the audio and a "clean" text input to predict the original, disfluent output.FileDescriptionKey ColumnNotestrain.csvLabeled examples for training the model.transcript (Disfluent)Crucial: The "clean" training transcript must be programmatically created from this file using the unique_disfluencies.csv list.test.csvInput for generating predictions.transcript (Clean)Contains the pre-cleaned text and the corresponding audio ID.unique_disfluencies.csvA helper file listing all unique disfluent words/phrases (e.g., "‡§Ö‡§Ç", "‡§π‡§Æ‡•ç‡§Æ").disfluencyUsed to create the clean training labels.downloaded_audios/Directory containing all audio files (.wav format), named after the sample id.N/AMulti-modal input source.Evaluation MetricThe submissions are evaluated using the Word Error Rate (WER), the standard metric for assessing ASR systems.$$\text{WER} = \frac{\text{Substitutions} + \text{Deletions} + \text{Insertions}}{\text{Number of Words in Reference}}$$Goal: To minimize the WER.Range: $0.0$ to $\infty$ (a score of $0.0$ is a perfect prediction). Lower is better.
